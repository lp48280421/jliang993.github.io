<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="stylesheet" href="../jemdoc.css" type="text/css" />
<title>Pricipal Component Pursuit</title>
<!-- MathJax -->
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
	  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<!-- End MathJax -->
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Jingwei Liang</div>
<div class="menu-item"><a href="../index.html">Home</a></div>
<div class="menu-category">Research</div>
<div class="menu-item"><a href="../publications.html">Publication</a></div>
<div class="menu-item"><a href="../activities.html">Activity</a></div>
<div class="menu-item"><a href="../codes.html">Codes</a></div>
<div class="menu-category">Teaching</div>
<div class="menu-item"><a href="../nsopt.html">Non-smooth<br /> Optimisation</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Pricipal Component Pursuit</h1>
</div>
<p>In this project, we will apply several classic operator splitting schemes to solve the Principal Component Pursuit (PCP, a.k.a. Robust Principal Component Analysis) problem, proposed by CandeÌ€s, Li, Ma and Wright, 2009. </p>
<h3>1. Forward model</h3>
<p>Suppose we have the following observation </p>
<p style="text-align:center">
\[
\begin{equation}\label{eq:forward}
f = x + y + \epsilon  ,
\end{equation}
\]
</p><p>where \(x \in \mathbb{R}^{m \times n}\) is a sparse matrix, \(y \in \mathbb{R}^{m \times n}\) is a low-rank matrix and \(\epsilon \in \mathbb{R}^{m \times n}\) is white Gaussian noise. The goal is to recover the sparse and low-rank components from the observation \(f\). There are various formulations of this decomposition, in the following we mainly focus on several convex approaches. </p>
<h3>2. Noiseless case</h3>
<p>When there is no noise, i.e. \(\epsilon=0\), the decomposition problem can be achieved via the following constrained convex optimisation problem</p>
<p style="text-align:center">
\[
\begin{equation}\label{eq:pcp-1}
\min_{x , y \in \mathbb{R}^{m \times n}} &nbsp; \mu \|{x}\|_1 +  \|{y}\|_* 
\quad
\textrm{s.t.} \quad  
x + y  = f .
\end{equation}
\]
</p><p>For the above problem, there are two methods can be applied: Douglas&ndash;Rachford splitting and Backward&ndash;Backward splitting with parameter continuation. Throughout the section, consider </p>
<p style="text-align:center">
\[
\begin{equation*}
\mu  = \tfrac{1}{ \sqrt{\max \{m, n\}} } .
\end{equation*}
\]
</p><h4>2.1 Douglas&ndash;Rachford splitting</h4>
<p>Let's first define several notations</p>
<p style="text-align:center">
\[
\begin{equation}\label{eq:lift}
z = \begin{pmatrix} x \\ y \end{pmatrix} \in \mathbb{R}^{2m \times n} ,\quad
R(z) = \mu \|{x}\|_1 +  \|{y}\|_* ,\quad
A = \begin{bmatrix} \mathrm{Id} &amp; \mathrm{Id} \end{bmatrix} .
\end{equation}
\]
</p><p>Then \(\eqref{eq:pcp-1}\) can be written as</p>
<p style="text-align:center">
\[
\begin{equation}\label{eq:pcp-2}
\min_{z \in \mathbb{R}^{2m \times n}} &nbsp; R(z)
\quad
\textrm{s.t.}
\quad
A z  = f .
\end{equation}
\]
</p><p>Define the set \(\Omega :=  \{ z \in \mathbb{R}^{2m \times n} : A z = f \}\), then we further get</p>
<p style="text-align:center">
\[
\begin{equation}\label{eq:pcp-3}
\min_{z \in \mathbb{R}^{2m \times n}} &nbsp; R(z) + \iota_{\Omega}(z)  ,
\end{equation}
\]
</p><p>which can be handled by Douglas&ndash;Rachford splitting easily. Let \(z = \begin{pmatrix} z_1 \\ z_2 \end{pmatrix}\)</p>
<ul>
<li><p>The proximal mapping of \(R\)</p>
</li>
</ul>
<p style="text-align:center">
\[
	\begin{equation*}
	\mathrm{prox}_{\gamma R} (z)
	=
	\begin{pmatrix}  \mathrm{prox}_{\gamma \|{\cdot}\|_1} (z_1) \\  \mathrm{prox}_{\gamma\mu \|{\cdot}\|_*} (z_2)  \end{pmatrix}  .
	\end{equation*}
	\]
</p><ul>
<li><p>The projection mapping onto \(\Omega\)</p>
</li>
</ul>
<p style="text-align:center">
\[
	\begin{equation*}
	\mathrm{proj}_{\Omega} (z) = z + A^T (AA^T)^{-1}(f - Az)   .
	\end{equation*}
	\]
</p><p>Therefore, the iteration of Douglas&ndash;Rachford the reads: let \(\gamma &gt; 0\), \(z_{1,0} = 0, z_{2,0} = f\) and \(v = \mathrm{proj}_{\Omega} (z_0)\)</p>
<p style="text-align:center">
\[
\begin{equation*}
\begin{aligned}
u_{1,k+1} &amp;= \mathcal{T}_{\gamma\mu} ( 2v_{1,k} - z_{1,k} ) 	\\
u_{2,k+1} &amp;= U \mathcal{T}_{\gamma} ( \Sigma ) V^T	\\
z_{1,k+1} &amp;= z_{1,k} + u_{1,k+1} - v_{1,k}	\\
z_{2,k+1} &amp;= z_{2,k} + u_{2,k+1} - v_{2,k}	\\
v_{k+1} &amp;= \mathrm{proj}_{\Omega} (z_{k+1})  ,
\end{aligned}
\end{equation*}
\]
</p><p>where \(\mathcal{T}_{\gamma}(\cdot)\) is the soft-thresholding operation and \(U\Sigma V^T\) is the SVD of \(2v_{2,k} - z_{2,k}\).</p>
<h3>3. Noisy case</h3>
<p>When \(\epsilon \neq 0\), then instead of the constrained optimisation problem, we need to consider the following regularised least square</p>
<p style="text-align:center">
\[
\begin{equation}\label{eq:pcp-7}
\min_{x, y \in\mathbb{R}^{m\times n}}&nbsp; \nu \left({ \tfrac{1}{ \sqrt{ \max \{m,n\} } } \|{x}\|_1 +  \|{y}\|_* }\right)  + \tfrac{1}{2}\|{ x + y - f }\|^2  ,
\end{equation}
\]
</p><p>where \(\nu &gt; 0\) is tradeoff parameter.
Follow the definitions in eqref{eq:lift}, we obtain the following simpler formulation</p>
<p style="text-align:center">
\[
\begin{equation}\label{eq:pcp-8}
\min_{z \in\mathbb{R}^{2m\times n}}&nbsp; \nu R(z)  + \tfrac{1}{2}\|{ Az - f }\|^2  ,
\end{equation}
\]
</p><p>which can be easily handled by Forward&ndash;Backward splitting and FISTA.</p>
<p>Now again, by applying the Moreau envelope trick, \(\eqref{eq:pcp-7}\) is equivalent to</p>
<p style="text-align:center">
\[
\begin{equation}\label{eq:pcp-9}
\min_{x \in \mathbb{R}^{m\times n}}&nbsp; \tfrac{\nu}{ \sqrt{ \max \{m,n\} } } \|{x}\|_1 + { ^1\left({ \nu \|{\cdot}\|_1}\right)(f-x) }  ,
\end{equation}
\]
</p><p>where \({ ^1\left({ \nu \|{\cdot}\|_1}\right)(f-x) }\) is the Moreau envelope parameterised by \(1\), hence its gradient is \(1\)-Lipschitz.</p>
<h3>4. MATLAB implementation</h3>
<p><b>Generating \(f\) based on \(\eqref{eq:forward}\)</b>  	</p>
<div class="codeblock">
<div class="blockcontent"><pre>
n = [32, 32]; <span class="comment">% dimension of matrix</span>

<span class="comment">% low-rank matrix</span>
r = 4;
xl0 = <span class="statement">rand</span>(n(1),r)*diag(r:-1:1)*<span class="statement">rand</span>(r,n(2));
xl0 = xl0/<span class="statement">max</span>(abs(xl0(:))) *50;

<span class="comment">% sparse matrix, sparsity = 0.2</span>
xs0 = <span class="statement">rand</span>(n)*<span class="statement">rand</span>(n);
xs0 = xs0/<span class="statement">max</span>(abs(xs0(:))) *50;

ratio = 0.2;
mask = proj_mask(xs0, ratio, <span class="string">'p'</span>);
xs0 = xs0 .* mask;

<span class="comment">% exact observation</span>
f0 = xs0 + xl0;

<span class="comment">% generate noise</span>
sigma = 1e-2*std(f0(:));

<span class="comment">% noisy observation</span>
f = xs0 + xl0 + sigma*<span class="statement">randn</span>(n); <span class="comment">% comment this line <span class="statement">for</span> the exact scenario</span>
</pre></div></div>
<p>Details of the codes can be found <a href="nsopt/src_Project2.zip" target=&ldquo;blank&rdquo;>here</a>. In the following, we mainly demonstrate the numerical performance comparison. </p>
<h4>4.2 Exact case and Douglas&ndash;Rachford splitting</h4>
<p>The numerical comparison of <i>Douglas&ndash;Rachford and its inertial versions</i> is illustrated in the figure below.</p>
<ul>
<li><p><b>1-DR</b> inertial parameter \(a = [0.4, 0]\)</p>
</li>
<li><p><b>2-DR</b> inertial parameter \(a = [0.5, -0.1]\)</p>
</li>
</ul>
<p>Example for which two inertial versions are almost the same.</p>
<table class="imgtable"><tr><td>
<img src="PCP_exact_DR-bad.png" alt="" height="450px" />&nbsp;</td>
<td align="left"></td></tr></table>
<p>Example for which \(2\)-step inertial DR is much faster under the same parameter choices. </p>
<table class="imgtable"><tr><td>
<img src="PCP_exact_DR-good.png" alt="" height="450px" />&nbsp;</td>
<td align="left"></td></tr></table>
<h4>4.3 Inexact case and Forward&ndash;Backward splitting/FISTA</h4>
<p>Numerical comparison of <i>Forward&ndash;Backward and FISTA</i>. </p>
<table class="imgtable"><tr><td>
<img src="PCP_inexact.png" alt="" height="450px" />&nbsp;</td>
<td align="left"></td></tr></table>
<div id="footer">
<div id="footer-text">
Page generated 2019-02-25, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
