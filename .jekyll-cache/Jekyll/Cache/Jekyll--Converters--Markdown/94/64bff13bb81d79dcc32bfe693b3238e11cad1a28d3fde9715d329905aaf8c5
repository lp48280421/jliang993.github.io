I"(<h4 id="description">Description</h4>
<p>The course covers topics in non-smooth optimization and first-order proximal splitting methods. This includes gradient based methods (gradient and subgradient method, proximal gradient method, accelerated gradient methods), operator splitting methods (augmented Lagrangian method, alternating direction method of multipliers, monotone operators and operator splitting), and (possibly) interior-point algorithms. Non-covnex optimisation and stochastic optimisation will also be introduced.</p>

<!-- ##### Lectures
Lectures are Monday and Wednesday, 11:00–12:00am, at MR9, Pav. B0.43.  -->

<h5 id="lecture-slides">Lecture slides</h5>
<ul>
  <li><a href="nsopt/slides/introduction.pdf">Introduction</a></li>
  <li><a href="nsopt/slides/lecture-01.pdf">Gradient method</a></li>
  <li><a href="nsopt/slides/lecture-02.pdf">Proximal gradient method</a></li>
  <li><a href="nsopt/slides/lecture-03.pdf">Krasnosel’skii-Mann iteration</a></li>
  <li><a href="nsopt/slides/lecture-04.pdf">Backward–Backward splitting</a></li>
  <li><a href="nsopt/slides/lecture-05.pdf">Douglas–Rachford splitting</a></li>
  <li><a href="nsopt/slides/lecture-06.pdf">Primal–Dual splitting</a></li>
  <li><a href="nsopt/slides/lecture-07.pdf">Other operator splitting methods</a></li>
  <li><a href="nsopt/slides/lecture-08.pdf">Alternating direction method of multipliers</a></li>
  <li><a href="nsopt/slides/lecture-09.pdf">Non-convex optimisation</a></li>
  <li><a href="nsopt/slides/lecture-10.pdf">Stochastic optimisation</a></li>
</ul>

<p><strong>Acknowledgement:</strong> some slides are based on the lecture slides of <a href="https://web.stanford.edu/~boyd/">Prof. Stephen Boyd</a> (Stanford) and <a href="http://www.seas.ucla.edu/~vandenbe/">Prof. Lieven Vandenberghe</a> (UCLA).</p>

<h5 id="projects">Projects</h5>
<ul>
  <li><a href="nsopt/projects/project-01.pdf">Project 1</a> Compare gradient descent, heavy-ball method and Nesterov’s acceleration scheme, and their proximal versions. (<a href="nsopt/project1">Instructions</a>, <a href="nsopt/projects/data.zip">data</a>, <a href="nsopt/src_Project1.zip">MATLAB code</a>)</li>
  <li><a href="nsopt/projects/project-02.pdf">Project 2</a> Principal component pursuit. (<a href="nsopt/project2">Instructions</a>, <a href="nsopt/projects/src_Project2.zip">MATLAB code</a>)</li>
</ul>

<table class="tg" style="width:96%;table-layout:fixed;border:1px solid rgb(222,222,222); margin-left:auto;margin-right:auto;">
      <tr>
        <th style="border:1px solid rgb(222,222,222);">Lecture slides</th>
        <th style="width:100%;border:1px solid rgb(222,222,222);">Projects</th>
      </tr>
      
</table>

<h5 id="references">References</h5>
<ul>
  <li>S. Boyd and L. Vandenberghe. Convex optimization. Cambridge university press, 2004.</li>
  <li>R. T. Rockafellar. Convex analysis. Princeton university press, 2015.</li>
  <li>A. Beck. First-order methods in optimization. Vol. 25. SIAM, 2017.</li>
  <li>H. H. Bauschke and P. L. Combettes. Convex analysis and monotone operator theory in Hilbert spaces. Vol. 408. New York: Springer, 2011.</li>
  <li>B. Polyak. Introduction to optimization. Optimization Software, 1987.</li>
  <li>Y. Nesterov. Introductory lectures on convex optimization: A basic course. Vol. 87. Springer Science &amp; Business Media, 2013.</li>
</ul>

:ET